# Streaming Data Loader

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org)
[![Kafka](https://img.shields.io/badge/Kafka-Streaming-red)](https://kafka.apache.org/)
[![Elasticsearch](https://img.shields.io/badge/Elasticsearch-Search-yellow)](https://www.elastic.co/elasticsearch/)
[![Prometheus](https://img.shields.io/badge/Monitoring-Prometheus-orange)](https://prometheus.io/)
[![Grafana](https://img.shields.io/badge/Dashboards-Grafana-brightgreen)](https://grafana.com/)
[![AsyncIO](https://img.shields.io/badge/Async-Enabled-lightgrey)](https://docs.python.org/3/library/asyncio.html)
[![CI](https://img.shields.io/badge/Tests-Pytest-green)](https://docs.pytest.org)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://www.docker.com/)
[![Kubernetes](https://img.shields.io/badge/K8s-Supported-blueviolet)](https://kubernetes.io/)
[![License](https://img.shields.io/badge/License-MIT-lightgrey)](LICENSE)

---

## Overview

**Streaming Data Loader** is a high-performance, async-powered microservice for processing data streams from **Kafka**
and bulk-loading them into **Elasticsearch**. It combines modern Python tooling with observability best practices to
provide **reliable, scalable, and debuggable data pipelines**.

Itâ€™s fast, resilient, and production-ready â€” ideal for those who need lightweight alternatives to complex ETL systems.

---

## Key Features

- **Asynchronous processing** with `asyncio`, `aiokafka`, and `aiohttp`
- **Batch insertions** for throughput efficiency
- **Retry & fault-tolerant** logic for Kafka and Elasticsearch
- **Configurable** via `.env` and `pydantic-settings`
- **Docker & Kubernetes ready**
- **Prometheus + Grafana** monitoring included
- **Tested** with `pytest`, including integration scenarios

---

## Quick Start

### ğŸ³ Docker Compose

```bash
docker-compose up --build
```

- http://localhost:9090 â†’ Prometheus
- http://localhost:3000 â†’ Grafana (admin / admin)
- http://localhost:8080 â†’ Kafka UI

### â˜¸ï¸ Kubernetes

#### Step 1 â€” Deploy

```bash
./deploy.sh
```

#### Step 2 â€” Cleanup

```bash
./clean.sh
```

---

## Architecture

This project uses **Hexagonal Architecture** (Ports and Adapters), ensuring modularity, extensibility, and clean separation of concerns.

```text
Kafka -â†’ KafkaConsumerService -â†’ EventService -â†’ ElasticsearchClientService -â†’ Elasticsearch
                         â”‚                â†“
                         â””-â†’ Metrics + Logging (Prometheus + JSON logs)
```

### Layers

- **Input Ports**: Kafka Consumer (aiokafka), deserialization, batching
- **Application Core**: Event transformation, validation, retry logic
- **Output Ports**: Async Elasticsearch client, bulk insert, failure handling
- **Infrastructure**: Docker, Kubernetes, logging, metrics, monitoring

---

## ğŸ” Why Choose This Over Logstash, Flume, etc.?

- âœ… **True async** data pipeline â€” lower latency, better throughput
- âœ… **No heavyweight config DSL** â€” Python code, `pyproject.toml`, `.env`
- âœ… **Built-in retries & fault handling** â€” robust out of the box
- âœ… **JSON logging** and metric labeling for full observability
- âœ… **Open-source & customizable** â€” perfect for modern data teams

---

## Observability

Prometheus scrapes metrics on `/metrics` (port `8000`). Dashboards are automatically provisioned in Grafana.

| Metric                              | Description                        |
|-------------------------------------|------------------------------------|
| `messages_processed_total`          | Total number of processed messages |
| `errors_total`                      | Total errors during processing     |
| `consume_duration_seconds`          | Time spent reading from Kafka      |
| `response_duration_seconds`         | Time to insert into Elasticsearch  |
| `transform_duration_seconds`        | Time spent transforming messages   |
| `batch_processing_duration_seconds` | Full batch processing time         |

---

## Testing

```bash
pytest -v
```

Includes:

- âœ… Unit tests
- âœ… Integration tests (Kafka â†’ ES)
- âœ… Metrics verification
- âœ… Config validation

---

## Technologies

- `Python 3.12` + `asyncio`
- `Kafka + aiokafka`
- `Elasticsearch` `Bulk API`
- `Pydantic` `dotenv` `poetry`
- `Prometheus` `Grafana`
- `Docker` `docker-compose`
- `Kubernetes-ready`
- JSON logging (`python-json-logger`)

---

## Project Structure

```text
streaming-data-loader/
â”œâ”€â”€ configs/                     # Prometheus / Grafana
â”œâ”€â”€ src/                         # Main source code
â”‚   â”œâ”€â”€ domain/
â”‚   â”œâ”€â”€ ports/
â”‚   â”œâ”€â”€ services/                # Event processing logic
â”‚   â”œâ”€â”€ config.py                # Settings & env config
â”‚   â”œâ”€â”€ logger.py                # JSON logger setup
â”‚   â””â”€â”€ metrics.py               # Prometheus metrics
â”œâ”€â”€ tests/                       # Unit & integration tests
â”œâ”€â”€ k8s/                         # Kubernetes manifests
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ deploy.sh / clean.sh
â””â”€â”€ pyproject.toml
```

---

## If you find this useful...

...give it a star, fork it, or mention it in your next data project!

## Author

**Anatoly Dudko**  
[GitHub @aDudko](https://github.com/aDudko) â€¢ [LinkedIn](https://www.linkedin.com/in/dudko-anatol/)
